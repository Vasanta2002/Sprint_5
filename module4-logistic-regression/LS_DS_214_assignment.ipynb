{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z45AFkqtDVmP"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-J6Mt_OTDVmS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H6-cYzjDVmT"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2YLircVDVmT"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vIvkhwfhDkSm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_5lH6cKMDVmU"
      },
      "outputs": [],
      "source": [
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL'])\n",
        "   \n",
        "    #Encode binary topings\n",
        "\n",
        "    binary_cols = ['Unreliable', 'NonSD', 'Beef', 'Pico', 'Guac', 'Cheese', 'Fries',\n",
        "       'Sour cream', 'Pork', 'Chicken', 'Shrimp', 'Fish', 'Rice', 'Beans',\n",
        "       'Lettuce', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage', 'Sauce',\n",
        "       'Salsa.1', 'Cilantro', 'Onion', 'Taquito', 'Pineapple', 'Ham',\n",
        "       'Chile relleno', 'Nopales', 'Lobster', 'Queso', 'Egg', 'Mushroom',\n",
        "       'Bacon', 'Sushi', 'Avocado', 'Corn', 'Zucchini', 'Chips']\n",
        "\n",
        "\n",
        "    df[binary_cols] = df[binary_cols].applymap(lambda x: 1 if type(x)==str else 0)\n",
        "    \n",
        "    #create psudo one-hot encoded categories for \"Burrito\"\n",
        "\n",
        "    burrito_type = ['california', 'asada', 'surf', 'carnitas']\n",
        "\n",
        "    for b in burrito_type:\n",
        "      df[b] = df['Burrito'].str.lower().str.contains(b).astype(int)\n",
        "\n",
        "\n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['Rec', 'overall', 'Burrito'])\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OKwikoNDVmU"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rLsH7g36DVmU"
      },
      "outputs": [],
      "source": [
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "df = wrangle(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Beef'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpXnDkGtEQZ3",
        "outputId": "3b273f5b-2036-43be-99dc-b408e1d18d8e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2016-01-18    1\n",
              "2016-01-24    1\n",
              "2016-01-24    0\n",
              "2016-01-24    1\n",
              "2016-01-27    1\n",
              "Name: Beef, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "x59y3jWAUwjH",
        "outputId": "85a4371d-254f-43fd-afdc-331d6a5ccbef"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Neighborhood  Yelp  Google  Chips  Cost  Hunger  Mass (g)  \\\n",
              "Date                                                                   \n",
              "2016-01-18      Miramar   3.5     4.2      0  6.49     3.0       NaN   \n",
              "2016-01-24   San Marcos   3.5     3.3      0  5.45     3.5       NaN   \n",
              "2016-01-24          NaN   NaN     NaN      0  4.85     1.5       NaN   \n",
              "2016-01-24          NaN   NaN     NaN      0  5.25     2.0       NaN   \n",
              "2016-01-27     Carlsbad   4.0     3.8      1  6.59     4.0       NaN   \n",
              "\n",
              "            Density (g/mL)  Length  Circum  ...  Bacon  Sushi  Avocado  Corn  \\\n",
              "Date                                        ...                                \n",
              "2016-01-18             NaN     NaN     NaN  ...      0      0        0     0   \n",
              "2016-01-24             NaN     NaN     NaN  ...      0      0        0     0   \n",
              "2016-01-24             NaN     NaN     NaN  ...      0      0        0     0   \n",
              "2016-01-24             NaN     NaN     NaN  ...      0      0        0     0   \n",
              "2016-01-27             NaN     NaN     NaN  ...      0      0        0     0   \n",
              "\n",
              "            Zucchini  Great  california  asada  surf  carnitas  \n",
              "Date                                                            \n",
              "2016-01-18         0      0           1      0     0         0  \n",
              "2016-01-24         0      0           1      0     0         0  \n",
              "2016-01-24         0      0           0      0     0         1  \n",
              "2016-01-24         0      0           0      1     0         0  \n",
              "2016-01-27         0      1           1      0     0         0  \n",
              "\n",
              "[5 rows x 63 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f07e42d-c8b6-4ec4-86d0-98b58802f994\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Yelp</th>\n",
              "      <th>Google</th>\n",
              "      <th>Chips</th>\n",
              "      <th>Cost</th>\n",
              "      <th>Hunger</th>\n",
              "      <th>Mass (g)</th>\n",
              "      <th>Density (g/mL)</th>\n",
              "      <th>Length</th>\n",
              "      <th>Circum</th>\n",
              "      <th>...</th>\n",
              "      <th>Bacon</th>\n",
              "      <th>Sushi</th>\n",
              "      <th>Avocado</th>\n",
              "      <th>Corn</th>\n",
              "      <th>Zucchini</th>\n",
              "      <th>Great</th>\n",
              "      <th>california</th>\n",
              "      <th>asada</th>\n",
              "      <th>surf</th>\n",
              "      <th>carnitas</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-18</th>\n",
              "      <td>Miramar</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0</td>\n",
              "      <td>6.49</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>San Marcos</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0</td>\n",
              "      <td>5.45</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>4.85</td>\n",
              "      <td>1.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-27</th>\n",
              "      <td>Carlsbad</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1</td>\n",
              "      <td>6.59</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 63 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f07e42d-c8b6-4ec4-86d0-98b58802f994')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f07e42d-c8b6-4ec4-86d0-98b58802f994 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f07e42d-c8b6-4ec4-86d0-98b58802f994');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbar80chDVmV"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6BYLDe0MDVmV"
      },
      "outputs": [],
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q0s_5tzDVmV"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8I-lK81iDVmW"
      },
      "outputs": [],
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\\\n",
        "# burrito_type = ['california', 'asada', 'surf', 'carnitas']\n",
        "\n",
        "# for b in burrito_type:\n",
        "#   df[b] = df['Burrito'].str.lower().str.contains(b).astype(int).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4a-Jl7zDVmW"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IB-BhrslDVmW"
      },
      "outputs": [],
      "source": [
        "target = 'Great'\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAPM76XJDVmW"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Pw_0h4UHDVmW"
      },
      "outputs": [],
      "source": [
        "cutoff = '2018'\n",
        "mask = X.index < cutoff\n",
        "X_train, y_train = X.loc[mask], y.loc[mask]\n",
        "X_test, y_test = X.loc[~mask], y.loc[~mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t9dHvqxDVmX"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts(normalize=True).max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "660LLuHNegUk",
        "outputId": "7d16ce7f-943e-45c1-cf66-e52c4e6b3459"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5822454308093995"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lamdBSXIDVmX",
        "outputId": "37719be9-89f7-4abc-c20b-b29df56fbd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy Score: 0.5822454308093995\n"
          ]
        }
      ],
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIB6FcQDVmX"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CF6jJc5DVmX",
        "outputId": "f6dc4374-3d56-4d0f-c89c-552d7dd8a2aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=['Neighborhood', 'Reviewer'],\n",
              "                               use_cat_names=True)),\n",
              "                ('simpleimputer', SimpleImputer()),\n",
              "                ('standardscaler', StandardScaler()),\n",
              "                ('logisticregression', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model_logr = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()   \n",
        ")\n",
        "\n",
        "model_logr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClDlOa95DVmY"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCqsDtHPDVmY",
        "outputId": "5d927297-71ba-4d26-dfab-fae342dc1781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MAE: 0.97911227154047\n",
            "Test MAE: 0.7894736842105263\n"
          ]
        }
      ],
      "source": [
        "training_acc = model_logr.score(X_train, y_train)\n",
        "test_acc = model_logr.score(X_test, y_test)\n",
        "\n",
        "print('Training MAE:', training_acc)\n",
        "print('Test MAE:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woXAvqaTDVmY"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KtBlFy-FDVmY"
      },
      "outputs": [],
      "source": [
        "# Create your horizontal barchart here.\n",
        "coefficients = model_logr.named_steps['logisticregression'].coef_[0]\n",
        "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
        "feat_imp = pd.Series(coefficients, index=features).sort_values(key=abs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(features), len(coefficients)"
      ],
      "metadata": {
        "id": "7VVILNo2uslQ",
        "outputId": "85f3857e-298a-4fb1-cdef-414ae9ea8002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(89, 87)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_imp.tail(10).plot(kind='barh')"
      ],
      "metadata": {
        "id": "tthVRLgguGp6",
        "outputId": "5a9ddbde-3b56-402b-9a21-b47eeaade432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c51a5fc10>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ajOJplDVmY"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfxyGlv2DVmY",
        "outputId": "b83ef010-76d7-4097-9f67-83f0bf7046de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "model_logr.predict_proba(X_test).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp5UY6xADVmY"
      },
      "source": [
        "**Give your written answer here:**\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LS_DS_214_solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}